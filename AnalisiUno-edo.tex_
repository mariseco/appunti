\chapter{Equazioni differenziali}

\section{classificazione}

Le equazioni differenziali sono una classe di \myemph{equazioni funzionali} ovvero
equazioni in cui l'incognita non è un numero (come accade nelle equazioni algebriche) ma è una funzione. La funzione incognita $u$, sarà quindi funzione di una variabile indipendente $u=u(t)$. Ad esempio $u$ potrebbe essere la traiettoria di un proiettile che è quindi una posizione nello spazio in funzione del tempo $t$. Se nelle equazioni algebriche il nome di gran lunga più utilizzato per l'incognita è $x$, nelle equazioni funzionali a seconda dei contesti le convenzioni possono cambiare in maniera drastica. Si dovrà utilizzare un nome per la funzione e un nome per la sua variabile indipendente: $x=x(t)$, $y=y(x)$, $y=y(t)$, $u=u(x)$ sono alcune delle scelte più utilizzate.
Se l'incognita è una funzione le operazioni che possono comparire nell'equazioni sono, oltre le usuali operazioni algebriche che agiscono sui singoli valori dell'equazione, anche operatori che agiscono sulla funzione in sé. Se l'equazione funzionale oltre alle operazioni algebriche comprende anche l'operatore derivata, si dirà che è una \mynote{equazione differenziale}%
\index{equazione!differenziale}
\emph{equazione differenziale}.
Di contro ci potranno ad esempio essere equazioni che coinvolgono l'operatore integrale e si chiameranno
\emph{equazioni integrali}.

Ci sono quindi diversi concetti che possono aiutare a classificare le equazioni differenziali. Se la funzione incognita è funzione di una singola variabile reale si dirà che l'equazione è una
\emph{equazione differenziale ordinaria}
\mynote{ODE}
\index{equazione!differenziale!ordinaria}
(abbreviato
\emph{EDO} \index{EDO}
in italiano,
\emph{ODE} \index{ODE} per gli anglosassoni).
Di contro se la funzione incognita è funzione di più variabili l'equazione si chiamerà
\emph{equazione differenziale alle derivate parziale}
\mynote{PDE}
\index{equazione!differenziale!alle derivate parziali}
 (abbreviato \emph{EDP} \index{EDP} in italiano e \emph{PDE}
 \index{PDE} in lingua inglese).
In questo corso, centrato sulle funzioni di una variabile, tratteremo quindi solamente le equazioni differenziali ordinarie.
L'\emph{ordine}
\mynote{ordine}
\index{ordine!equazione differenziale}
\index{equazione!differenziale!ordine}
dell'equazione differenziale è il numero massimo di derivate successive che vengono applicate alla funzione incognita.
Tipicamente la funzione $u$ sarà definita su un intervallo $I$ della retta reale: $u\colon I \to \RR$ e la forma più generale di equazione differenziale ordinaria di ordine $n$ si potrà dunque scrivere come:
\begin{equation}\label{eq:3784643}
  F(t,u(t),u'(t),u''(t), \dots, u^{(n)}(t)) = 0.
\end{equation}
con $F\colon \Omega \to \RR$ una funzione data, definita su un insieme $\Omega \subset I\times \RR^{n+1}$.
Una funzione $u\colon I \to \RR$ si dice essere una \emph{soluzione dell'equazione differenziale} \eqref{eq:3784643} se $u$ è derivabile almeno $n$ volte in ogni punto $t\in I$ e se \eqref{eq:3784643} è soddisfatta
per ogni $t\in I$.

Usualmente si tende a semplificare la notazione evitando di scrivere sempre esplicitamente il punto $t$ in cui viene calcolata la funzione. Sarà quindi usuale
scrivere l'equazione \eqref{eq:3784643}
nella forma abbreviata:
\[
  F(t,u,u',u'', \dots, u^{(n)}) = 0
\]
rendendo anche più evidente il fatto che l'incognita è $u$,  l'intera funzione, e non un singolo valore $u(t)$.


Se ad esempio scegliamo $n=2$ e $F(t,u,v,z)= z+\sin u + v$ otteniamo l'equazione differenziale:
\[
 u''(t) + \sin u(t) + u'(t) = 0
\]
che, in opportune unità di misura, è l'equazione del moto di un pendolo smorzato, dove $t$ rappresenta il tempo e $u$ la misura dell'angolo di inclinazione del pendolo rispetto alla verticale.
Osserviamo che nell'esempio precedente la funzione $F$ non dipende direttamente dalla variabile $t$. Equazioni con questa proprietà si dicono
\emph{equazioni autonome}
\mynote{equazioni autonome}
\index{equazione!differenziale!autonoma}
ed è immediato osservare che se $u(t)$ è soluzione anche una sua traslazione temporale $v(t) = u(t-t_0)$ è soluzione dell'equazione (il moto del pendolo non dipende dall'ora in cui si svolge).

Una equazione scritta nella forma \eqref{eq:3784643} si dice \myemph{equazione in forma implicita} e per analogia con le equazioni algebriche (si pensi all'equazione  $u^2(t) + t^2 = 1$) ci si aspetta che le soluzioni di tale equazioni siano meglio rappresentate da curve piuttosto che da grafici di funzione.
Risulta in effetti che la teoria delle equazioni differenziali si applica con molta maggiore efficacia alle \myemph{equazioni in forma normale} che sono le equazioni differenziali di ordine $n$ che possono essere scritte
esplicitando la dipendenza dalla derivata di ordine massimo:
\begin{equation}\label{eq:366793}
 u^{(n)}(t) = f(t, u(t), u'(t), \dots, u^{n-1}(t))
\end{equation}
dove $f\colon \Omega \to \RR$ è una funzione definita su $\Omega\subset I\times \RR^n$.
L'equazione del pendolo si può scrivere in questa forma,
scegliendo $f(t,u,v) = -\sin u - v$.

Più in generale potremmo considerare
\emph{sistemi di equazioni differenziali}
\mynote{sistemi}%
\index{sistemi di equazioni differenziali}%
\index{equazione!differenziale!sistema}%
in più incognite.
Possiamo rappresentare un sistema di $k$ equazioni ordinarie in $m$ incognite nella forma:
\begin{equation}\label{eq:375456}
  \vec F(t, \vec u(t), \vec u'(t), \dots, \vec u^{n}(t)) = 0
\end{equation}
dove $\vec u$ è una funzione $\vec u \colon I \to \RR^m$ le cui componenti sono le $m$ funzioni incognite:
\[
  \vec u(t) = (u_1(t), \dots, u_m(t))
\]
mentre la funzione $\vec F \colon I \times (\RR^m)^{n+1} \to \RR^k$ è stavolta una funzione a valori vettoriali $\vec F = (F_1, \dots, F_k)$ cosicché l'equazione vettoriale
\eqref{eq:375456} è effettivamente equivalente ad un sistema di $k$ equazioni:
\[
\begin{cases}
F_1(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
F_2(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
\quad\vdots \\
F_k(t,\vec u(t), \vec u'(t)\dots, \vec u^{(n)}(t)) = 0\\
\end{cases}
\]
Vedremo che per le equazioni ordinarie del primo ordine è naturale, come accade per le equazioni algebriche, avere lo stesso numero di equazioni e di incognite dunque è tipico avere singole equazioni scalari del primo ordine
(cioè in cui l'incognita è una funzione a valori nel campo
degli scalari $\RR$) o sistemi di $k=m$ equazioni del primo ordine con incognita una funzione vettoriale $\vec u$ (cioè una funzione a valori nello spazio vettoriale $\RR^m$) ovvero con $m$ incognite $u_1,\dots, u_m$ funzioni scalari.

Una importante osservazione è il fatto generale che una equazione differenziale di ordine $n$ può essere ricondotta ad un sistema di $n+1$ equazioni differenziali del primo ordine.
Basta infatti considerare come incognita il vettore (chiamato \emph{Jet})
\[
  \vec u = (u, u', u'', \dots, u^{n})
\]
comprendente tutte le derivate della funzione scalare $u$.
L'equazione \eqref{eq:3784643}, di ordine $n$, risulta infatti equivalente al sistema di $n+1$ equazioni del primo ordine nella variabile $\vec u = (u_0, \dots, u_n)$
\[
  \begin{cases}
    F(t, u_0(t), u_1(t), \dots, u_n(t)) = 0\\
    u_1(t) = u_0'(t)\\
    u_2(t) = u_1'(t)\\
    \quad \vdots \\
    u_n(t) = u_{n-1}'(t)
  \end{cases}
\]
Nel caso, più interessante,
delle equazioni in forma normale consideriamo un vettore
$\vec u = (u_0,\dots, u_{n-1})$
con $n$ componenti corrispondenti alle derivate di $u$:
\[
  \vec u = (u, u', \dots, u^{(n-1)})
\]
cosicché \eqref{eq:366793}, l'equazione normale di ordine $n$
diventa un sistema di $n$ equazioni
del primo ordine
in $n$ incognite
\[
  \begin{cases}
  u_{n-1}'(t) = f(t,u_0(t), u_1(t), \dots, u_{n-1}(t)) \\
  u_0'(t) = u_1(t)\\
  u_1'(t) = u_2(t)\\
  \quad \vdots\\
  u_{n-2}'(t) = u_{n-1}(t)
  \end{cases}
\]
ovvero una equazione differenziale vettoriale
\[
  \vec u'(t) = \vec f(t, \vec u(t))
\]
avendo definito $\vec f = (f_0, \dots, f_{n-1})$
come
\begin{gather*}
  f_0(t, x_0,\dots, x_{n-1}) = x_1 \\
  f_1(t, x_0,\dots, x_{n-1}) = x_2 \\
  \quad \vdots \\
  f_{n-2}(t, x_0,\dots, x_{n-1}) = x_{n-1} \\
  f_{n-1}(t, x_0,\dots, x_{n-1}) = f(t,x_0, \dots, x_{n-1})
\end{gather*}

Nell'esempio del pendolo si avrà come incognita una funzione vettoriale $\vec u(t) = (u(t),u'(t))$ le cui componenti sono posizione e velocità angolare. Il codominio di tale funzione si chiama \emph{spazio delle fasi}. L'equazione (essendo autonoma tralasciamo la dipendenza da $t$) si scriverà nella forma $\vec u' = \vec f(\vec u)$ con $\vec f(x,v) = -\sin(x) - v$.

Un caso molto particolare ma decisamente importante è quello in cui la funzione $F$ (per le equazioni in forma implicita) o la funzione $f$ (per le equazioni in forma normale) sono funzioni lineari
per ogni $t$
rispetto alla variabile $u$.
In tal caso diremo che l'equazione è
\mynote{equazioni lineari omogenee}%
\index{equazioni differenziali!lineari omogenee}%
\emph{lineare omogenea}.
Più precisamente si avrà
\[
F(t,u(t),u'(t), \dots, u^{(n)}(t)) = A_t(u(t), u'(t), \dots, u^{(n)}(t))
\]
con $A_t\colon \RR^{n+1}\to \RR$
operatore lineare per ogni $t$ ovvero $A_t$ si rappresenta tramite un vettore i cui coefficienti sono funzioni della variabile $t$:
\[
  A_t(v) = \sum_{k=0}^n a_k(t) v_k
\]
e l'equazione differenziale si scrive nella forma
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = 0.
\]
Nel caso in cui i coefficienti $a_k(t)$ non dipendano da $t$ (cioè siano funzioni costanti) diremo che l'equazione è
lineare
\mynote{coefficienti costanti}%
\index{equazioni differenziali!lineari a coefficienti costanti}%
\emph{a coefficienti costanti}.

E' facile osservare che l'insieme delle soluzioni di una equazione lineare omogenea è uno spazio vettoriale: $u=0$ è sempre soluzione, se $u$ è una soluzione e $\lambda \in \RR$ anche $\lambda u$ è soluzione e se $u$ e $v$ sono due soluzioni anche $u+v$ è soluzione
\mymargin{principio di sovrapposizione}
\index{principio di sovrapposizione}
(principio di sovrapposizione).

In effetti
se le funzioni $u$ sono definite su un intervallo $I$ possiamo
identificare $F$ con un funzionale $L\colon \RR^I \to \RR^I$ definito da
\[
  L(u)(t) = F(t,u(t), u'(t), \dots, u^{(n)}(t))
\]
Se l'equazione differenziale è lineare allora $L$ è un operatore lineare sullo spazio vettoriale $\RR^I$ e lo spazio delle soluzioni dell'equazione differenziale non è altro che il $\ker L$
ed è noto che $\ker L$ è un sottospazio vettoriale.

Nel caso in cui la funzione $F$ (o la corrispondente $f$)
sia affine si dirà che l'equazione differenziale
è una equazione
\mynote{equazione lineari}%
\index{equazioni differenziali!lineari}%
\emph{lineare (non omogenea)}.
L'equazione non omogenea avrà la forma:
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = g(t).
\]
Se $v_0$ e $v_1$ sono due soluzioni di questa equazione è chiaro che la differenza $u=v_1 - v_0$ è soluzione dell'equazione omogenea
\[
  a_0(t) u(t) + a_1(t) u'(t) + \dots + a_n(t) u^{(n)}(t) = 0.
\]
Dunque quest'ultima si chiama equazione omogenea associata alla non omogenea e se $v_0$ è una soluzione particolare (qualunque) dell'equazione non omogenea ogni soluzione $v$ dell'equazione non omogenea si scrive nella forma
\[
  v = v_0 + u
\]
con $u$ soluzione dell'omogenea associata. Per trovare tutte le soluzioni di una equazione non omogenea è dunque sufficiente trovare una soluzione particolare e tutte le soluzioni della equazione omogenea associata.

L'equazione del pendolo non è lineare
ma quando l'angolo $u$ è piccolo (cioè per \emph{piccole oscillazioni}) si ha $\sin u \sim u$.
Facendo questa \emph{linearizzazione} si ottiene l'equazione
\[
  u''(t)  + u(t) + u'(t) = 0.
\]
Questa è una equazione lineare omogenea. Se sul pendolo agisce una forza esterna (pendolo forzato) l'equazione diventa
\[
  u''(t) + u(t) + u'(t) = g(t)
\]
dove $g(t)$ rappresenta l'entità di una forza esterna variabile nel tempo. Questa equazione è lineare non omogenea.

Come nel caso generale le equazioni lineari di ordine $n$ si riconducono a sistemi lineari di $n$ equazioni del primo ordine.

\section{alcuni risultati preparatori sulle funzioni vettoriali e di più variabili}

Ci sarà utile estendere la definizione delle classi di regolarità $C^k$ alle funzioni vettoriali di più variabili.

Se $\Omega\subset \RR^m$ è un insieme aperto e $f\colon \Omega \to \RR$ è una funzione, possiamo definire le \emph{derivate parziali} di $f$. Se $\vec x \in \Omega$ si ha $\vec x = (x_1,\dots, x_m)$ e quindi $f(\vec x) = f(x_1,\dots, x_m)$. Se facciamo variare una sola componente $x_j$ mantenendo fisse tutte le altre componenti, otteniamo una funzione di una variabile $x_j \mapsto g(x_j) = f(x_1,\dots, x_j, \dots, x_m)$.
Definiamo dunque la \myemph{derivata parziale} di $f$ rispetto alla variabile $x_j$ come la derivata della funzione $g(x_j)$. Denotiamo tale derivata con il simbolo:
\[
  \frac{\partial f}{\partial x_j}.
\]
Le derivate parziali si calcolano dunque come le usuali derivate per le funzioni di una variabile, solamente bisogna considerare costanti tutte le variabili tranne quella coinvolta nella derivazione. Ad esempio se $f(x,v) = -\sin(x) - v$ (come nell'esempio fatto nell'introduzione) si ha
\begin{align*}
  \frac{\partial f}{\partial x} &= -\cos(x)\\
  \frac{\partial f}{\partial v} &= -1.
\end{align*}

Se $\vec f\colon \Omega \subset \RR^m \to \RR^n$ è una funzione di più variabili a valori vettoriali potremo scrivere $\vec f = (f_1, \dots, f_n)$ e considerare le derivate parziali di ogni componente:
\[
  \frac{\partial \vec f}{\partial x_j}
  = \enclose{\frac{\partial f_1}{\partial x_j}, \dots, \frac{\partial f_n}{x_j}}.
\]

\begin{definition}
Sia $\Omega\subset \RR^m$ un aperto. Denoteremo con $C^0(\Omega,\RR^m)$ lo spazio vettoriale di tutte le funzioni
continue $f\colon \Omega \to \RR^n$.

Denotiamo con $C^1(\Omega,\RR^m)$ lo spazio vettoriale
delle funzioni in $C^0(\Omega,\RR^m)$ tali che ognuna delle loro derivate parziali $\frac{\partial f}{\partial x_j}$ è a sua volta una funzione continua (cioè in $C^0(\Omega,\RR^m)$).
Ricorsivamente si definisce $C^k(\Omega,\RR^m)$ per $k>1$,
come lo spazio delle funzioni $\Omega\to\RR^m$ tali che tutte le loro derivate parziali stanno in $C^{k-1}(\Omega,\RR^m)$.
\end{definition}

\begin{theorem}[disuguaglianza di Cauchy-Schwarz]
Siano $\vec v, \vec w \in \RR^m$. Allora
\begin{equation}\label{eq:cauchy_schwarz}
   (\vec v,\vec w) \le \abs{\vec v}\cdot \abs{\vec w}.
\end{equation}
\end{theorem}
%
\begin{proof}
Si ha
\[
 0 \le \abs{\vec v-\vec w}^2 = (\vec v-\vec w,\vec v-\vec w) = \abs{\vec v}^2 - 2(\vec v,\vec w) + \abs{\vec w}^2
\]
da cui la disuguaglianza (disuguaglianza di Young):
\[
  (\vec v,\vec w) \le \frac{\abs{\vec v}^2 + \abs{\vec w}^2}{2}.
\]
Nel caso particolare $\abs{\vec v}=\abs{\vec w}=1$ (in tal caso si dice che $\vec v$ e $\vec w$ sono versori) si ottiene
\[
  (\vec v,\vec w) \le 1.
\]
Applichiamo dunque la precedente disuguaglianza ai versori $\vec v/\abs{\vec v}$ e $\vec w/\abs{\vec w}$ per ottenere
\[
  \frac{(\vec v,\vec w)}{\abs{\vec v}\cdot \abs{\vec w}} =
  \enclose{\frac{\vec v}{\abs{\vec v}},\frac{\vec w}{\abs{\vec w}}} \le 1.
\]
Se $\abs{\vec v}\neq 0$ e $\abs{\vec w}\neq 0$ moltiplicando ambo i membri per $\abs{\vec v}\cdot\abs{\vec w}$ si ottiene~\eqref{eq:cauchy_schwarz}. In caso contrario ambo i membri della disuguaglianza~\eqref{eq:cauchy_schwarz} sono nulli e la disuguaglianza è banalmente verificata.
\end{proof}

\begin{definition}
Sia $\vec f\colon [a,b]\to \RR^m$, $\vec f(x) = (f_1(x), \dots, f_m(x))$.
Diremo che $\vec f$ è integrabile su $[a,b]$ se ogni
$f_k\colon[a,b]\to \RR$ è integrabile su $[a,b]$ e in tal caso porremo:
\[
  \int_a^b \vec f(x)\, dx = \enclose{\int_a^b f_1(x)\, dx, \dots, \int_a^b f_m(x)\, dx}
\]
cosicché per ogni $k=1,\dots, m$ si ha:
\[
  \enclose{\int_a^b \vec f(x)\, dx}_k = \int_a^b f_k(x)\, dx.
\]
Come al solito si pone inoltre $\int_b^a \vec f = - \int_a^b \vec f$.
\end{definition}

\begin{theorem}\label{teo:tipo_jensen}
Sia $\vec f\colon [a,b]\to \RR^m$ integrabile. Allora si ha
\[
  \abs{\int_a^b \vec f(x)\, dx} \le \int_a^b \abs{\vec f(x)}\, dx.
\]
\end{theorem}
%
\begin{proof}
Posto
\[
 \vec v = \int_a^b \vec f(x)\, dx
\]
si ha, usando la linearità dell'integrale
e sfruttando la disuguaglianza di Cauchy-Schwarz
\begin{align*}
  \abs{\vec v}^2
  &= (\vec v,\vec v)
   = \sum_{k=1}^n v_k \int_a^b f_k(x)\, dx
   = \int_a^b \sum_{k=1}^m v_k f_k(x)\, dx \\
  &= \int_a^b \enclose{\vec v, \vec f(x)}\, dx
  \le \int_a^b \abs{\vec v}\cdot \abs{\vec f(x)}\, dx
  = \abs{\vec v} \int_a^b \abs{\vec f(x)}\, dx.
\end{align*}
Se $\abs{\vec v}\neq 0$ possiamo dividere ambo i membri per $\abs{\vec v}$ e ottenere la disuguaglianza cercata.
Altrimenti se $\abs{\vec v}=0$ la disuguaglianza è certamente soddisfatta in quanto il lato destro non può essere negativo.
\end{proof}

\section{Il problema di Cauchy}
Il problema di Cauchy consiste nel
trovare una soluzione di un sistema di $n$ equazioni differenziali ordinarie del primo ordine in $n$ incognite
con un dato iniziale fissato. Cioè dato $t_0\in \RR$,
$\vec u_0\in \RR^n$ si cerca un intervallo $I\subset \RR$ con $t_0\in I$ e una funzione $\vec u \colon I\to \RR^n$
che sia derivabile e che soddisfi:
\begin{equation}\label{eq:problema_cauchy}
\begin{cases}
 \vec u'(t) = \vec f(t, \vec u(t)), \qquad \forall t\in I\\
 \vec u(t_0) = \vec u_0.
\end{cases}
\end{equation}

\begin{theorem}[Cauchy-Lipschitz, esistenza e unicità]
Sia $\Omega$ un aperto di $\RR\times \RR^n$ e sia $\vec f\colon \Omega \to \RR^n$, una funzione continua e tale che esista $L>0$ per cui vale
\[
  \abs{\vec f(t,\vec u_1) - \vec f(t,\vec u_2)} \le L \abs{\vec u_1 - \vec u_2}
  \]
per ogni $t\in \RR$, $\vec u_1, \vec u_2\in \RR^n$
con $(t,\vec u_1),(t,\vec u_2)\in\Omega$
(diremo che $f$ è Lipschitziana in $\vec u$ uniformemente rispetto a $t$).

Dato $(t_0,\vec u_0)\in \Omega$
esiste $\delta_0>0$ tale che per ogni $\delta < \delta_0$
posto $I_\delta = [t_0-\delta, t_0+\delta]$ esiste una unica funzione $\vec u\colon I_\delta \to \RR^n$ tale che $(t,\vec u(t))\in \Omega$ per ogni $t\in I_\delta$, $u$ è derivabile
e soddisfa il problema di Cauchy~\eqref{eq:problema_cauchy}.
\end{theorem}
%
\begin{proof}
Innanzitutto vogliamo trasformare il problema differenziale in un problema integrale. Se $\vec u$ è una funzione derivabile soluzione di \eqref{eq:problema_cauchy}
allora è chiaro che $\vec u'(t) = \vec f(t,\vec u(t))$ è continua in quanto composizione di funzioni continue e dunque $\vec u$ è di classe $C^1$. Possiamo dunque integrare tra $t_0$ e $t$ i due lati dell'equazione differenziale per ottenere:
\[
  \vec u(t) - \vec u(t_0) = \int_{t_0}^t \vec f(s,\vec u(s))\, ds
\]
e dunque se $\vec u$ risolve il problema di Cauchy allora $\vec u$ soddisfa anche la seguente equazione integrale:
\begin{equation}\label{eq:cauchy_integrale}
  \vec u(t) = \vec u_0 + \int_{t_0}^t \vec f(s,\vec u(s))\, ds.
\end{equation}
Viceversa se $\vec u$ è continua e soddisfa \eqref{eq:cauchy_integrale} allora si ha ovviamente
$\vec u(t_0) = \vec u_0$ e, passando alle derivate, si scopre che $\vec u$ è derivabile e soddisfa l'equazione differenziale $\vec u'(t) = \vec f(t,\vec u(t))$.
Dunque trovare una soluzione $C^1$ del problema di Cauchy~\eqref{eq:problema_cauchy} è equivalente a trovare una soluzione $C^0$ del problema integrale~\eqref{eq:cauchy_integrale}.
Ci dedicheremo dunque a questo secondo problema.

Denotiamo con
\[
  C_{\alpha,\beta} =
  \{(t,\vec u)\in \RR\times\RR^n\colon \abs{t-t_0}\le \alpha, \abs{\vec u -\vec u_0}\le \beta\}
\]
il cilindro centrato in $(t_0, \vec u_0)$ con asse parallelo all'asse delle $t$, di altezza $2\alpha$ e raggio $\beta$.
Visto che $\Omega$ è aperto esiste una palla aperta centrata in $(t_0,\vec u_0)$ e contenuta in $\Omega$. Tale palla dovrà anche contenere un cilindro $C_{\alpha,\beta}$ per un qualche $\alpha>0$ e $\beta>0$. Il cilindro $C_{\alpha,\beta}$ è chiuso e limitato in $\Omega$ e dunque $\vec f$, essendo continua, è limitata su $C_{\alpha,\beta}$ per il teorema di Weierstrass. Sia $M>0$ tale che $\abs{\vec f(t,\vec u)}\le M$ per ogni $(t,\vec u) \in C_{\alpha,\beta}$ e definiamo
\begin{equation}\label{eq:395109}
 \delta = \min\{\alpha, \frac{\beta}{M}, \frac{1}{2L}\}.
\end{equation}
Sia $I=[t_0-\delta,t_0+\delta]$ e
sia
\[
X=\{\vec u\in C^0(I,\RR^n)\colon \Abs{\vec u-\vec u_0}_\infty
\le \beta \}.
\]
Sappiamo che $C^0(I,\RR^n)$ è uno spazio metrico completo e $X$ è chiuso quindi anch'esso è completo. Possiamo allora considerare l'operatore $T\colon X \to C^0(I, \RR^n)$:
\[
  T(\vec u) = \vec u_0 + \int_{t_0}^t \vec f(s, \vec u(s))\, ds.
\]
Vogliamo innanzitutto verificare che $X$ è invariante, cioè che $T(X)\subset X$. Ma se $\vec u \in X$ e se $t\in I$ si ha
$(t,\vec u(t)) \in C_{\delta,\beta} \subset C_{\alpha,\beta}$
e dunque (usando anche~\eqref{eq:395109} e il teorema~\ref{teo:tipo_jensen})
\[
  \abs{\vec T(u)(t)-\vec u_0}
  = \abs{\int_{t_0}^t \vec f(s,\vec u(s))\, ds}
  \le \int_{t_0}^t M\, ds = \abs{t-t_0} M \le \delta M
  \le \beta.
\]
Dunque $\Abs{T(\vec u)-\vec u_0}\le \beta$ e dunue $T(\vec u)\in X$.
Abbiamo dunque che $T\colon X \to X$. Verifichiamo ora che $T$ è una contrazione. Si ha:
\begin{align*}
  \abs{T(\vec u_1)(t)-T(\vec u_2)(t)}
   &\le \abs{\int_{t_0}^t\abs{\vec f(s,\vec u_1(s))-\vec f(s,\vec u_2(s))}\, ds}\\
   &\le \abs{\int_{t_0}^t L\abs{\vec u_1(s)-\vec u_2(s)}\, ds}\\
   &\le L \abs{t-t_0}\Abs{\vec u_1 - \vec u_2}_\infty
\end{align*}
da cui per~\eqref{eq:395109}
\[
 \Abs{T(\vec u_1)-T(\vec u_2)}_\infty \le L \delta \Abs{\vec u_1 - \vec u_2}_\infty \le \frac{1}{2}\Abs{\vec u_1- \vec u_2}.
\]
Dunque $T\colon X \to X$ è una contrazione su uno spazio metrico completo. Per il teorema di punto fisso di Banach-Caccioppoli
sappiamo quindi che esiste una unica funzione $\vec u \in X$ ovvero $\vec u \colon I \to \RR^n$ che soddisfa $T(\vec u)=\vec u$ ovvero~\eqref{eq:cauchy_integrale} ovvero~\eqref{eq:problema_cauchy}.
\end{proof}

\begin{proposition}
Il teorema precedente si applica in particolare se
$\vec f \in C^1(\Omega, \RR^n)$.
\end{proposition}
%
\begin{proof}
Se prendiamo un cilindro $K=C_{\alpha,\beta}\subset \Omega$ centrato in $(t_0,\vec u_0)$, sappiamo che ogni derivata parziale $\partial \vec f / \partial u_j$ è continua ed è quindi limitata (per il teorema di Weierstrass) su $K$. Sia $L_j$ il massimo di $\abs{\partial \vec f / \partial u_j}$ su $K$ e sia $L$ il massimo degli $L_j$. Allora presi $\vec u, \vec v \in \RR^n$ con $(t,\vec u),(t,\vec v)\in K$ si può scomporre l'incremento vettoriale $\vec v-\vec u$ lungo le $n$ direzioni coordinate e applicare il teorema di Lagrange lungo ogni direzione.
Per ogni $k=1,\dots, n$ si ha quindi:
\begin{align*}
\MoveEqLeft
\abs{f_k(t,\vec v)-f_k(t,\vec u)}\\
&\le \sum_{j=1}^n \abs{f_k(t,v_1, \dots, v_j, u_{j+1}, \dots, u_n) - f_k(t,v_1, \dots, v_{j-1}, u_j, \dots, u_n)}\\
&\le \sum_{j=1}^n L_j \abs{v_j-u_j}
\le \sum_{j=1}^n L_j \abs{v-u}
\le n L \abs{v-u}
\end{align*}
da cui
\begin{align*}
  \abs{\vec f(t,\vec v) - \vec f(t,\vec u)}
  &= \sqrt{\sum_{k=1}^n \abs{\vec f(t,\vec v) - \vec f(t,\vec u)}^2} \\
  &\le \sqrt{\sum_{k=1}^n n^2 L^2 \abs{v-u}^2} \\
  &\le n\sqrt{n} L \abs{v-u}.
\end{align*}
Abbiamo quindi mostrato che per ogni $(t_0,\vec u_0)\in \Omega$ esiste un intorno del punto $(t_0,\vec u_0)$ in cui la funzione $\vec f$ soddisfa le ipotesi del teorema precedente.
\end{proof}

\begin{comment}

\begin{theorem}[maggiore regolarità delle soluzioni]
Sia $\vec u(x)$ una soluzione del sistema di
equazioni differenziali:
\[
  \vec u'(x) = \vec f(x, \vec u(x)).
\]
Se $\vec f\colon \Omega\subset \RR\times\RR^n \to\RR^n$ è una funzione di classe $C^k$, con $k\in \NN$ allora la soluzione $\vec u(x)$ è di classe $C^{k+1}$.
In particolare se $\vec f \in C^\infty$ anche $u\in C^\infty$.
\end{theorem}
%
\begin{proof}
Se $\vec u$ soddisfa l'equazione differenziale significa quanto meno che $\vec u$ è derivabile. Dunque è continua e derivabile e la derivata è
\[
  \vec u'(x) = \vec f(x,\vec u(x)).
\]
Ma il lato destro è composizione di funzioni continue (in quanto al minimo $\vec f \in C^0$ e $u\in C^0$) dunque la derivata di $u$ è di classe $C^0$. Significa che come minimo $\vec u$ è di classe $C^1$. Se ora $k\ge 1$ possiamo ripetere il procedimento e osservare che $\vec u'(x)$ è uguale ad una composizione di funzioni $C^1$, dunque è di classe $C^1$. Ma allora $\vec u$ è di classe $C^2$.
Il procedimento può essere dunque iterato fino ad arrivare a dedurre che $\vec u$ è di classe $C^k$. A quel punto la funzione $f(x,\vec u(x))$ è di classe $C^k$ e quindi $\vec u'$ è di classe $C^k$ e $\vec u$ è di classe $C^{k+1}$. Non si può iterare ulteriormente perché ora componendo una funzione $C^k$ con una funzione $C^{k+1}$ quello che ottengo è di nuovo $C^k$.

Dire che $f\in C^\infty$ significa che $f\in C^k$ per ogni $k$. Dunque $\vec u$ risulta essere di classe $C^{k+1}$ per ogni $k$: dunque diremo che $\vec u$ è di classe $C^\infty$.
\end{proof}
\end{comment}

\begin{definition}[estensione di una soluzione, soluzione massimale]
Sia $I\subset \RR$ un intervallo e sia $\vec u \colon I \to \RR^n$ una soluzione dell'equazione differenziale in forma normale del primo ordine:
\begin{equation}\label{eq:edo_normale_ordine_uno}
  \vec u'(x) = \vec f(x,\vec u(x))
\end{equation}
dove $\vec f\colon \Omega \subset \RR \times \RR^n \to \RR^n$
 e $\Omega$ è un aperto di $\RR \times \RR^n$.

 Se $J\supset I$, $J \neq I$ è un intervallo e se $\vec v\colon J \to \RR^n$ risolve la stessa equazione~\eqref{eq:edo_normale_ordine_uno}
 e se $\vec v(x)=\vec u(x)$ per ogni $x\in I$, diremo che $\vec v$ è una \myemph{estensione della soluzione} $u(x)$.

 Se la soluzione $\vec u$ non ammette estensioni, diremo che $\vec u$ è una soluzione definita su un intervallo massimale o, più semplicemente, diremo che $\vec u$ è una \myemph{soluzione massimale}.
\end{definition}

\begin{proposition}[caratterizzazione delle soluzioni massimali]
Supponiamo che $\vec f\colon \Omega\subset \RR\times\RR^n\to\RR^n$
soddisfi le ipotesi del teorema di Cauchy-Lipschitz.
Allora una soluzione $\vec u$ di \eqref{eq:edo_normale_ordine_uno}
è definita su un intervallo massimale $I$ se e solo se
$I$ è aperto e per ogni $K$ compatto $K\subset \Omega$
e per ogni $x_0 \in I$ esistono $x_1,x_2\in I$, $x_1 < x_0 < x_2$ tali che $\vec u(x_1)\not \in K$ e $\vec u(x_2) \not \in K$.
Significa cioè che il grafico di $\vec u$ cioè la curva $(x,\vec u(x))$ esce da qualunque compatto fissato $K\subset \Omega$ sia facendo crescere $x$ verso destra che facendo calare $x$ verso sinistra. In altre parole il grafico della soluzione massimale tende ad arrivare sulla frontiera di $\Omega$.
\end{proposition}
%
\begin{proof}
\emph{Prima implicazione}.
Supponiamo che $\vec u\colon I \to \RR^n$ sia una soluzione di~\eqref{eq:edo_normale_ordine_uno} definita su un intervallo massimale $I$. Dovrà essere $(x,u(x))\in \Omega$ per ogni $x\in I$.

Mostriamo innanzitutto
che $I$ è un intervallo aperto a destra: se non lo fosse si avrebbe che $x_0=\sup I \in I$. Allora $(x_0,u(x_0))\in \Omega$ e per il teorema di esistenza e unicità locale
possiamo trovare un intorno $J$ di $x_0$ e una funzione
$\vec v\colon J\to \RR^n$ che risolve~\eqref{eq:edo_normale_ordine_uno}. Per l'unicità delle soluzioni $\vec u$ e $\vec v$ devono coincidere su $I\cap J$ e dunque facendo l'unione dei due grafici ottengo una estensione di $\vec u$ a tutto l'intervall $I\cup J$ che è strettamente più grande di $I$. Dunque $\vec u$ non poteva essere una soluzione massimale.

Mostriamo ora che la curva $(x,\vec u(x))$ esce da ogni compatto $K\subset \Omega$ sia da destra che da sinistra. Supponiamo per assurdo che esista un compatto $K$ tale che $(x,\vec u(x))\in K$ per ogni $x\in I$.
Il grafico $(x,\vec u(x))$ è limitato per $x\in I$ dunque certamente $I$ deve essere limitato. Inoltre abbiamo visto che $I$ è aperto e quindi $I=(a,b)$ con $a,b\in \RR$, $a<b$.

Allora per ogni $x\in I=(a,b)$ si ha
\[
  \abs{\vec u'(x)} = \abs{\vec f(x, \vec u(x))}
   \le \sup_K \abs{\vec f}.
\]
Visto che $\vec f$ è continua (in quanto soddisfa le ipotesi del teorema di esistenza e unicità) la funzione $\abs{\vec f}$ ha massimo su $K$ per il teorema di Weierstrass e quindi esiste $M\ge 0$ tale che $\abs{\vec u'(x)} \le M$ per ogni $x\in (a,b)$. Significa che ogni componente di $\vec u$ è $M$-lipschitziana, in particolare
ogni componente è uniformemente continua. Dunque la funzione $\vec u$ può essere estesa con continuità ad una funzione $\vec v \colon [a,b] \to \RR^n$ definita anche agli estremi dell'intervallo.
Visto che $(x,\vec u(x)) \in K$ per ogni $x\in(a,b)$ e visto che $\vec v(b) = \lim_{x\to b^-}\vec u(x)$ essendo $K$ chiuso possiamo affermare che $\vec v(b) \in K$. E lo stesso vale per $\vec v(a)$.  particolare $(x,\vec v(x))\in K \subset \Omega$ per ogni $x\in [a,b]$. Vogliamo ora verificare che $\vec v$ è soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno}.
Chiaramente $\vec v$ soddisfa l'equazione per ogni $x\in (a,b)$
in quanto su $(a,b)$ coincide con $\vec u$ che è soluzione.
Consideriamo l'estremo $b$.
Visto che $\vec v$ è continua in $b$ e $\vec f$ è continua in $(b,\vec v(b))$ si ha
\[
  \lim_{x\to b^-} \vec v'(x)
  = \lim_{x\to b^-} \vec f(x,\vec  v(x))
  = \vec f(b,\vec v(b)) \in \RR^n
\]
Sappiamo però che se il limite della derivata di una funzione continua esiste ed è finito, allora la funzione è derivabile nel punto limite e la derivata è continua in quel punto (si applichi il teorema dell'Hospital al limite del rapporto incrementale).
Dunque $\vec v$ è derivabile in $b$ e anche in quel punto soddisfa l'equazione differenziale. Lo stesso vale nel punto $a$. Dunque siamo riusciti a trovare una estensione $\vec v$ di $\vec u$ contraddicendo l'ipotesi che $\vec u$ fosse una soluzione massimale.

\emph{Seconda implicazione}. Supponiamo ora $\vec u\colon I\to \RR^n$ sia una soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno} definita su un intervallo $I$ e tale che la curva $(x,\vec u(x))$ esca da ogni compatto $K$ al variare di $x\in I$. Vogliamo dimostrare che $\vec u$ è soluzione massimale. Innanzitutto $I$ non può essere compatto, altrimenti anche $K=\{(x,\vec u(x))\colon x \in I\}$ sarebbe un compatto di $\Omega$ e ovviamente la curva non esce da $K$.
Sia $a=\inf I$ e $b=\sup I$. Siccome $I$ non è chiuso esso non contiene uno dei due estremi: supponiamo sia $b$. Allora se $\vec u$ fosse estendibile a destra esisterebbe una estensione continua $\vec v\colon (a,b]\to \RR^n$ che coincide con $\vec u$ su $(a,b)$ e che è continua nel punto $x=b$ e che soddisfa l'equazione differenziale quindi, in particolare, $(b,\vec v(b))\in \Omega$.
Ma allora $(x,\vec u(x)) \to (b,\vec v(b)) \in \Omega$ e visto che $\Omega$ è aperto esiste un intervallino $[b-\eps, b]$ su cui $(x,\vec u(x)) \in \Omega$.
Se prendiamo $x_0 = b-\eps$ e $K=\{(x,\vec u(x))\colon x\in [x_0,b]\}$ abbiamo un compatto
ma per ogni $x\ge x_0$ si ha $(x,\vec u(x)) \in K$.
Il che è contro le ipotesi.
\end{proof}

\begin{proposition}[separazione delle soluzioni]
Se $\vec f\colon \Omega\subset \RR \times \RR^n\to \RR^n$ soddisfa le ipotesi del teorema di esistenza e unicità e se $\vec u\colon I \to \RR^n$ e $\vec v\colon I\to\RR^n$ sono due soluzioni
dell'equazione differenziale $\vec u'(x) = \vec f(x,\vec u(x))$
definite su due intervalli $I,J \subset \RR$  e se esiste $x_0\in I\cap J$ tale che $\vec u(x_0) = \vec v(x_0)$ allora $\vec u(x) = \vec v(x)$ per ogni $x\in I\cap J$.
Detto in altri termini: nelle ipotesi del teorema di esistenza e unicità i grafici di due soluzioni diverse definite in un intervallo non possono toccarsi.
\end{proposition}
%
\begin{proof}
Sia $x_0 \in I\cap J$ un punto in cui $\vec u(x_0) = \vec v(x_0)$
e supponiamo per assurdo che esista $x_2 \in I\cap J$ tale che $\vec u(x_2)\neq \vec u(x_2)$. In tal caso possiamo considerare il punto
\[
   x_1 = \sup \{x \in [x_0,x_2] \colon \vec u(x) = \vec v(x)\}.
\]
Su tutto l'intervallo $[x_0,x_1)$ si ha quindi $\vec u(x) = \vec v(x)$ e per continuità dovrà dunque anche essere $\vec u(x_1) =  \vec v(x_1)$.
In pratica $x_1$ è l'ultimo punto di contatto tra le due soluzioni.
Ponendo allora il punto $(x_1,\vec u(x_1))$ come dato iniziale del problema di Cauchy scopriamo che $\vec u$ e $\vec v$ sono localmente due soluzioni di tale problema. Per l'unicità locale le due soluzioni devono coincidere in un piccolo intorno del punto $x_1$, diciamo in particolare che devono coincidere su $[x_1,x_1+\eps]$ ma questo è in contraddizione con la definizione di $x_1$.
\end{proof}

\begin{theorem}[esistenza di soluzioni massimali]
Se $\vec u\colon I\to \RR^n$ è una soluzione dell'equazione differenziale~\eqref{eq:edo_normale_ordine_uno}
e se $\vec u$ non è essa stessa una soluzione massimale, esiste
sempre una estensione $\vec v\colon J\to \RR^n$ con $J\supset I$
che sia massimale.
\end{theorem}
%
\begin{proof}
Supponiamo che $\vec u$ non sia massimale. Sia $\F$ la famiglia di tutte le estensioni di $\vec u$. Per il teorema di esistenza e unicità, se due soluzioni coincidono in un punto allora devono coincidere dappertutto,

Vediamo innanzitutto quando è possibile estendere una soluzione.

Se $\vec u\colon I \to \RR^n$ è una soluzione significa che $(x,u(x))\in \Omega$ per ogni $x\in I$. Concentriamoci sulle estensioni dell'intervallo $I$ verso destra, ragionamento analogo si potrà fare verso sinistra. Sia dunque $b=\sup I$ l'estremo destro di $I$. Se $b\in I$ allora il problema di Cauchy con dato iniziale $u(b)$ nel punto $b$, ha una unica soluzione locale che quindi coincide con la soluzione $u(x)$ per $x\le b$ ed estende tale soluzione.
\end{proof}


\begin{theorem}[esistenza globale]
\label{th:edo_esistenza_globale}
Sia $I = (a,b) \subset \RR$ un intervallo aperto e sia
$\vec f\colon I \times \RR^n \to \RR^n$, $(x,\vec u)\mapsto \vec f(x,\vec u)$ una funzione che soddisfa le ipotesi di esistenza e unicità locale (dunque continua nella coppia $(x,\vec u)$ e lipschitziana nella variabile $\vec u$ uniformemente rispetto alla variabile $x$) e che soddisfi inoltre l'ipotesi aggiuntiva (sublinearità in $\vec u$ uniforme rispetto a $x$):
\[
  \abs{\vec f(x,\vec u)} \le m\abs{\vec u} + q.
\]
Allora per ogni $x_0\in I$ e per ogni $\vec u_0\in \RR^n$ esiste una unica funzione $u\colon I \to \RR^n$ soluzione del problema di Cauchy~\eqref{eq:problema_cauchy}.
\end{theorem}
%
\begin{proof}

\end{proof}

Il problema di Cauchy per le equazioni di ordine $n$ è il problema di determinare la soluzione di una equazione differenziale ordinaria di ordine $n$
in forma normale accoppiato ad una condizione iniziale per il valore della funzione e di tutte le sue derivate fino all'ordine $n$. Dato $\Omega\subset \RR\times \RR^n$ aperto, $f\in C^0(\Omega)$, $\vec y = (y_1, \dots, y_n) \in \RR^n$
si tratta quindi di trovare un intervallo $I\subset \RR$ e una funzione $u\in C^n(I)$ che soddisfi le seguenti condizioni:
\begin{equation}\label{eq:problema_cauchy_ordine_n}
  \begin{cases}
    u^{(n)}(x) = f(x,u(x), u'(x), \dots, u^{(n-1)}(x))\\
    u(x_0) = y_1 \\
    u'(x_0) = y_2 \\
    \ \vdots \\
    u^{(n-1)}(x_0) = y_n.
  \end{cases}
\end{equation}

\begin{theorem}[esistenza e unicità per le equazioni di ordine $n$]
Se $f\colon \Omega\to \RR$ soddisfa le ipotesi del teorema di esistenza e unicità per i sistemi del primo ordine (cioè $f$ è continua e lipschitziana nelle ultime $n-1$ variabili uniformemente rispetto alla prima) allora il problema di Cauchy~\eqref{eq:problema_cauchy_ordine_n} ammette una unica soluzione locale. Esiste cioè un $\delta_0>0$ tale che per ogni $\delta < \delta_0$ posto $I_\delta = [x_0-\delta,x_0+\delta]$ esiste una unica $u\in C^n(I_\delta)$ che soddisfa~\ref{eq:problema_cauchy_ordine_n}.

Se poi $\Omega$ è della forma $\Omega = I \times \RR^n$ con $I\subset \RR$ intervallo aperto e se
$f$ è anche sublineare (comme nelle ipotesi di esistenza globale) allora il problema di Cauchy~\ref{eq:problema_cauchy_ordine_n} ammette una unica soluzione definita su tutto $I$.
\end{theorem}
%
\begin{proof}
Se $u\in C^n$ è una funzione scalare possiamo considerare la funzione vettoriale $\vec u \in \C^1$ le cui componenti sono $u$ e le sue prime $n-1$ derivate:
\[
\vec u(x) = (u(x), u'(x), \dots, u^{(n-1)}(x))
\]
ovvero $u_k(x) = u^{(k-1)}(x)$ per $k=1,\dots ,n$ essendo $\vec u(x) = (u_1(x), \dots, u_n(x))$.

Con questa trasformazione il problema~\eqref{eq:problema_cauchy_ordine_n} si può scrivere nella forma:
\[
 \begin{cases}
   u_1'(x) = u_2 \\
   u_2'(x) = u_3 \\
   \ \vdots \\
   u_{n-1}'(x) = u_n \\
   u_n'(x) = f(x, u_1(x), u_2(x), \dots, u_n(x))\\\\
   u_1(x_0) = y_1\\
   \ \vdots \\
   u_{n}(x_0) = y_n
 \end{cases}
\]
ovvero posto $\vec f(x,\vec y) = (y_2, y_3, \dots, y_n, f(x, \vec y))$ abbiamo una funzione vettoriale $\vec f\colon \Omega \to \RR^n$ e il problema~\eqref{eq:problema_cauchy_ordine_n} risulta equivalente a
\begin{equation}\label{eq:437583}
  \begin{cases}
   \vec u'(x) = \vec f(x,\vec u(x))\\
   \vec u(x_0) = \vec y.
  \end{cases}
\end{equation}
Visto che $f$ è continua, anche $\vec f$ risulta continua.
Verifichiamo se $\vec f$ soddisfa la condizione di Lipschitz.
Per ipotesi $f$ la soddisfa, cioè
esiste $L>0$ tale che:
\[
  \abs{f(x,\vec y)-f(x,\vec z)} \le L\abs{\vec y - \vec z}.
\]
Ma allora si ha
\begin{align*}
\abs{\vec f(x,\vec y) - \vec f(x,\vec z)}
  &= \sqrt{\sum_{k=2}^n \abs{y_k-z_k}^2 + \abs{f(x,\vec y)-f(x,\vec z)}^2} \\
  &\le \sqrt{\abs{\vec y - \vec z}^2 + L^2 \abs{\vec y - \vec z}^2}
  = \sqrt{1+L^2}\cdot \abs{\vec y - \vec z}.
\end{align*}
Dunque la funzione $\vec f$ verifica le ipotesi del teorema di Cauchy\hyp{}Lipschitz: esiste dunque una soluzione $\vec u$ di tale problema in un opportuno intervallo centrato nel punto $x_0$.
Ponendo $u=u_1$ (la prima componente di $\vec u$)
si osserva che $u$ è di classe $C^n$.
Infatti sappiamo che $\vec u$ è di classe $C^1$
ed essendo
$u_1' = u_2$,
$u_2' = u_3$, \dots,
$u_{n-1}'=u_n$
ed essendo $u_n\in C^1$,
si scopre che $u=u_1$ è di classe $C^n$
ed è una soluzione del problema \eqref{eq:problema_cauchy_ordine_n}.
Anche l'unicità segue direttamente dall'equivalenza delle due formulazioni.

L'esistenza globale segue in maniera analoga dal teorema per i sistemi del primo ordine. Basti osservare che se la funzione $f$ soddisfa l'ipotesi di sublinearità anche $\vec f$ la soddisfa.
\end{proof}

\section{metodi risolutivi}

Una tipologia di equazioni differenziali che abbiamo già trattato
è data dalle equazioni della forma:
\[
   u'(x) = f(x).
\]
Banalmente l'insieme delle soluzioni è dato dalle primitive di $f$:
\[
  u(x) \in \int f(x)\, dx.
\]

Osserviamo che se $f$ è continua il problema di Cauchy associato
\[
  \begin{cases}
    u'(x) = f(x) \\
    u(x_0) = u_0
  \end{cases}
\]
ha una unica soluzione che si può scrivere nella forma:
\[
  u(x) = u_0 + \int_{x_0}^x f(t)\, dt
\]

\subsection{equazioni lineari del primo ordine}

Sono le equazioni del tipo:
\[
   u'(x) + a(x) u(x) = b(x).
\]
Per risolvere queste equazioni si cerca di ricondurre la somma al lato sinistro alla derivata di un prodotto.
Per fare ciò si considera una qualunque primitiva
$A(x) \in \int a(x)\, dx$ e si moltiplicano ambo i membri
per $e^{A(x)}$:
\[
  e^{A(x)} u'(x) + a(x) e^{A(x)} u(x) = b(x) e^{A(x)}
\]
essendo $A'(x) = a(x)$
si osserva che il lato sinistro è ora la derivata di un prodotto:
\[
  \enclose{e^{A(x)}u(x)}' = b(x) e^{A(x)}.
\]
Scelta una qualunque primitiva del lato destro
\[
  F(x) \in \int b(x) e^{A(x)}\, dx
\]
su ogni intervallo in cui $a(x)$ e $b(x)$ sono definite
si ha
\[
  e^{A(x)}u(x) = F(x) + c
\]
per qualche $c\in \RR$
in quanto $e^{A(x)}u(x)$ e $F(x)$ sono due primitive della stessa funzione.
Dunque
\[
  u(x) = e^{-A(x)}\enclose{F(x) + c}.
\]

Se $b(x)=0$ l'equazione è lineare omogenea, possiamo scegliere $F(x) = 0$ e quindi lo spazio delle soluzioni in questo caso
è dato da
\[
  u(x) = c e^{-A(x)}
\]
ed è quindi lo spazio vettoriale unidimensionale generato dalla funzione $e^{-A(x)}$.

Ogni soluzione della non omogenea si può scrivere come somma di una soluzione particolare
più una generica soluzione dell'equazione omogenea associata. Infatti:
\[
  u(x) = u_0(x) + c e^{-A(x)}.
\]
dove
\[
  u_0(x) = e^{-A(x)}F(x)
\]
è una particolare soluzione dell'equazione non omogenea.

Osserviamo che se $a(x)$ e $b(x)$ sono funzioni continue definite su uno stesso intervallo $I$, anche la soluzione è definita su tutto $I$. Si dirà quindi che la soluzione esiste \emph{globalmente}.

\begin{exercise}[autovettori dell'operatore derivata]
Fissato $\lambda \in \RR$ trovare tutte le soluzioni dell'equazione
\[
  u'(x) = \lambda u(x).
\]
\end{exercise}
%
\begin{proof}[Svolgimento]
Scriviamo l'equazione nella forma
\[
  u'(x) - \lambda u(x) = 0.
\]
Nelle notazioni precedenti abbiamo $a(x) = -\lambda$ e quindi possiamo scegliere $A(x) = -\lambda x \in \int a$.
Moltiplicando ambo i membri per $e^{-A(x)}$ si ottiene
\[
  e^{-\lambda x} u'(x) - \lambda e^{-\lambda x} u(x) = 0
\]
cioè
\[
 \enclose{e^{-\lambda x}\cdot u(x)}' = 0
\]
da cui su ogni intervallo in cui $u$ è definita esiste una costante $c$ tale che
\[
  e^{-\lambda x} u(x) = c
\]
ovvero
\[
  u(x) = c e^{\lambda x}.
\]
Abbiamo dunque trovato che le soluzioni sono definite su tutto $\RR$, una soluzione è $e^{\lambda x}$ e ogni altra soluzione è multiplo di questa.
\end{proof}

\begin{exercise}
\begin{enumerate}
\item
Trovare tutte le soluzioni dell'equazione differenziale
\[
  u'(x) - \frac{u(x)}{x} = x^2.
\]

\item
Trovare tutte le soluzioni dell'equazione differenziale
\[
  x u'(x) - u(x) = x^3.
\]
\end{enumerate}
\end{exercise}
\begin{proof}[Svolgimento]
La prima è una equazione lineare non omogenea del primo ordine in forma normale: $u'(x) + a(x) u(x) = b(x)$. Il fattore integrante è $e^{A(x)}$ con
\[
  A(x) \in \int a(x)\, dx = -\int \frac{1}{x}\, dx \ni - \ln \abs{x}.
\]
Dunque $e^{A(x)} = 1/\abs{x}$. Dovremmo dunque dividere ambo i membri dell'equazione per $\abs{x}$. Osserviamo che l'equazione non è definita per $x=0$ e possiamo dunque distinguere i casi $x>0$ e $x<0$. Decidiamo quindi, per semplicità, di cambiare segno all'equazione per $x<0$ cosicché possiamo dividere per $x$ invece che per $\abs{x}$. Si ottiene dunque:
\[
  \frac{u'}{x} - \frac{u}{x^2} = x
\]
cioè
\[
  \enclose{u\cdot \frac{1}{x}}'  = x
\]
da cui
\[
  \frac{u}{x} \in \int x\, dx \ni \frac{x^2}{2}.
\]
Dunque la funzione $u(x)/x$ differisce da $x^2/2$ per una costante su ognuno dei due intervalli $x>0$ e $x<0$. Su ognuno dei due intervalli si ha dunque:
\[
  \frac{u(x)}{x} = \frac{x^2}{2} + c
\]
da cui
\[
  u(x) = \frac{x^3}{2} + c x
\]
per qualche $c\in \RR$. Per come è stato posto il problema, la soluzione non deve essere definita per $x=0$ e la costante $c$ può essere quindi diversa se $x>0$ o $x<0$.

La seconda equazione è equivalente alla prima se $x\neq 0$. Ma non è in forma normale e le soluzioni
potranno essere definite anche per $x=0$.
Si avrà quindi
\[
  u(x) = \frac{x^3}{2} + c x
\]
come prima ma affinché la funzione sia derivabile in $x=0$ la costante $c$ dovrà essere uguale per $x>0$ e per $x<0$.

Si osservi che ogni soluzione soddisfa la condizione iniziale $u(0) = 0$ e che quindi nessuna soluzione soddisfa la condizione $u(0)= q$ se $q \neq 0$.
\end{proof}

\begin{exercise}
Risolvere l'equazione differenziale:
\[
 u'(x) + \frac{u(x)}{(1+x^2)\arctg x} = 1.
\]
\end{exercise}
%
\begin{proof}
Osserviamo che l'equazione è definita solo per $x\neq 0$. Cercheremo quindi le soluzioni sui due intervalli $x<0$ e $x>0$.
Moltiplicando ambo i membri dell'equazione per $\arctg x$ si ottiene
\[
  \arctg x \cdot u'(x) +\frac{1}{1+x^2} u(x) = \arctg x
\]
cioè:
\[
  \enclose{\arctg x \cdot u(x)}' = \arctg x
\]
da cui
\[
  \arctg x \cdot u(x) \in \int \arctg x\, dx \ni x \arctg x - \frac {1}{2}\ln(1+x^2).
\]
Dunque su ogni intervallo su cui la soluzione è definita esisterà
$c\in \RR$ tale che
\[
  \arctg x \cdot u(x) = x \arctg x - \frac{1}{2}\ln(1+x^2) + c
\]
da cui essendo $x\neq 0$ si può dividere per $\arctg x$ e ottenere
\[
  u(x) = x - \frac{\ln(1+x^2)}{2\arctg x} + \frac{c}{\arctg x}.
\]
Abbiamo quindi una famiglia di soluzioni definite per $x<0$ e una famiglia di soluzioni definite per $x>0$.
\end{proof}

\subsection{equazioni a variabili separabili}

Si chiamano equazioni a variabili separabili le
equazioni del tipo:
\begin{equation}\label{eq:edo_separabile}
  u'(x) = f(x) \cdot g(u(x)).
\end{equation}
Questa è una equazioni del primo ordine in forma normale:
\[
  u'(x) = F(x, u(x))
\]
dove nella funzione $F$ risulta possibile separare le variabili $x$ e $u$ in un prodotto:
\[
  F(x, u) = f(x)\cdot g(u).
\]

Se $u$ è una soluzione dell'equazione~\eqref{eq:edo_separabile}
e se $x$ è un punto in cui $g(u(x))\neq 0$, possiamo dividere ambo i membri dell'equazione per $g(u(x))$ per ottenere:
\[
  \frac{u'(x)}{g(u(x))} = f(x).
\]
Vogliamo ora scrivere il lato sinistro come la derivata della funzione composta. Se scegliamo una primitiva di $1/g$:
\[
  H(u) \in \int \frac{1}{g(u)}\, du
\]
si osserva che
\[
  \enclose{H(u(x))}' = H'(u(x))u'(x) = \frac{u'(x)}{g(u(x))} = f(x).
\]
dunque se $F\in \int f$, su ogni intervallo in cui $g(u(x))\neq 0$ dovrà esistere $c\in \RR$ tale che
\[
  H(u(x)) = F(x) + c.
\]
Se supponiamo inoltre che $H$ sia invertibile si avrà:
\[
  u(x) = H^{-1}(F(x)+ c).
\]

\begin{example}
Risolviamo l'equazione
\begin{equation}\label{eq:43856}
  u'(x) = x u^2(x) + x.
\end{equation}
E' una equazione del primo ordine in forma normale.
Raccogliendo $x$ al lato destro si ottiene una equazione a variabili separabili.
Dividendo ambo i membri per $u^2(x)+1$ (che è sempre diverso da zero) si ottiene l'equazione equivalente
\[
\frac{u'(x)}{1+u^2(x)} = x.
\]
Integrando il lato sinistro si ottiene:
\[
  \int \frac{u'(x)}{1+u^2(x)}\, dx
  = \Enclose{\int \frac{du}{1+u^2}}_{u=u(x)}
  \ni \arctg(u(x))
\]
mentre per il lato destro si ottiene
\[
  \int x\, dx \ni \frac{x^2}{2}.
\]
Dunque su ogni intervallo si deve avere
\[
  \arctg(u(x)) = \frac{x^2}{2} + c.
\]
Visto che l'arcotangente assume valori compresi tra $-\pi/2$ e $\pi/2$ anche il lato destro dovrà rimanere in tale intervallo. Dovrà quindi essere:
\begin{equation}\label{eq:4856}
  -\pi < x^2 + 2c < \pi.
\end{equation}
Con questa condizione possiamo invertire l'arcotangente ottenendo finalmente una espressione per la soluzione:
\[
  u(x) = \tg\enclose{\frac{x^2}{2} + c}.
\]

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{fig43856.png}
\label{fig:43856}
\caption{i grafici delle soluzioni dell'equazione differenziale~\eqref{eq:43856}}.
\end{figure}

Esplicitando la condizione~\eqref{eq:4856} si osserva che per $c>-\pi/2$ la soluzione è definita su un intervallo aperto centrato in $x=0$ mentre per $c\le -\pi/2$ la soluzione è definita su due intervalli simmetrici e l'ampiezza di tali intervalli si riduce tendendo a zero quando $c\to -\infty$.
Agli estremi di tali intervalli la soluzione ha degli asintoti verticali.

Significa dunque che anche le soluzioni massimali possono essere definite su intervalli arbitrariamente piccoli coerentemente con quanto affermato nel teorema di esistenza e unicità locale.
\end{example}

\begin{example}
Si voglia risolvere l'equazione
\[
  u'(x) = u^2(x).
\]
Si tratta di una equazione in forma normale, del primo ordine, autonoma. In particolare è a variabili separabili $u'(x) = f(u(x))\cdot g(x)$ con $f(u)=u^2$ e $g(x)=1$.
Osserviamo innanzitutto che $u(x) = 0$ è soluzione in quanto $u'(x) = u^2(x) = 0$. Se $u$ è una soluzione non identicamente nulla ci saranno dei punti in cui $u(x)\neq 0$. Nei punti in cui $u(x)\neq 0$ possiamo dividere ambo i membri dell'equazione per $u^2(x)$ ottenendo:
\[
  \frac{u'(x)}{u^2(x)} = 1.
\]
Osserviamo ora che, tramite cambio di variabile $u=u(x)$, $du=u'(x)\, dx$ si ottiene:
\[
  \int \frac{u'(x)}{u^2(x)}\, dx = \Enclose{\int \frac{du}{u^2}}_{u=u(x)} = \Enclose{-\frac{1}{u}}_{u=u(x)}
  = -\frac{1}{u(x)}
\]
mentre
\[
  \int 1 \, dx \ni x
\]
dunque su ogni intervallo in cui $u(x)\neq 0$ deve esistere una costante $c\in \RR$ tale che
\[
-\frac{1}{u(x)} = x + c
\]
ovvero, ponendo $x_0 = -c$
\begin{equation}\label{eq:5782196}
  u(x) = -\frac{1}{x+c} = \frac{1}{x_0-x}.
\end{equation}

Ci chiediamo ora se è possibile che una soluzione $u(x)$ possa avere sia dei punti in cui si annulla sia dei punti in cui non si annulla. La risposta è no, e può essere ottenuta in due modi diversi.

Giustificazione algebrica: sia $u(x)$ una soluzione qualunque, definita su un intervallo $I$ e supponiamo che ci sia un punto $x_0$ tale che $u(x_0)\neq 0$. Prendiamo il più grande intervallo $J\subset I$ contenente $x_0$ e tale che $u(x)\neq 0$ su $J$.
Tale intervallo deve essere aperto, perché se $u(x)\neq 0$ in un estremo dell'intervallo allora per continuità $u(x)$ sarebbe diverso da $0$ in un intorno di tale estremo e potrei quindi allargare l'intervallo. Dunque $J=(a,b)$ e se fosse $J\neq I$ si avrebbe che uno dei due estremi, diciamo $a$, è in $I$ e $u(a)=0$ (altrimenti potrei aggiungere l'estremo a $J$ che quindi non sarebbe massimale). Ma sempre per la continuità di $u$ si avrebbe $u(x) \to 0$ per $x\to a$ e $x\in J$ mentre nell'equazione~\eqref{5782196} vediamo che $u(x)\to 0$ è possibile solamente se $x\to +\infty$ o $\to -\infty$ che non sono certamente punti di $I$.

Giustificazione analitica: supponiamo che $u(x)$ sia una soluzione definita su un intervallo $I$ e supponiamo che $u(x_1)=0$ e $u(x_2)\neq 0$. Per semplicità supponiamo $x_1<x_2$ (dimostrazione analoga si fa nel caso inverso). Prendiamo $x_0 = \sup \{x \colon u(x) = 0, x \in [x_1,x_2]\}$. Chiaramente $x_0$ è limite di punti in cui $u(x)=0$ e dunque, per continuità, $u(x_0)=0$.
\end{example}

\section{equazioni lineari di ordine $n$}

\index{equazioni!differenziali!lineari di ordine $n$}
\mymargin{equazioni lineari di ordine $n$}
Le equazioni differenziali ordinarie lineari di ordine $n$ in forma normale possono essere scritte nella forma:
\begin{equation}\label{eq:edo_lineare_ordine_n}
  u^{(n)}(x) + a_{n-1}(x) u^{(n-1)}(x) + \dots + a_1(x) u'(x) + a_0 u(x) = b(x)
\end{equation}
con $a_k\colon A \to \RR$, $b\colon A\to \RR$ funzioni continue definite su uno stesso dominio $A\subset \RR$.
Nel caso $b(x) = 0$ l'equazione dice essere \myemph{omogenea}
e si può scrivere come:
\begin{equation}\label{eq:edo_lineare_omogenea_ordine_n}
  u^{(n)}(x) + a_{n-1}(x) u^{(n-1)}(x) + \dots + a_1(x) u'(x) + a_0 u(x) = 0.
\end{equation}
In generale l'equazione \eqref{eq:edo_lineare_ordine_n}
viene chiamata \myemph{equazione non omogenea} e la corrispondente equazione~\eqref{eq:edo_lineare_omogenea_ordine_n}
viene chiamata \myemph{equazione omogenea associata}.

\begin{theorem}[struttura delle soluzioni di una equazione lineare]
Siano $a_k\in C^0(A)$ con $A\subset \RR$ aperto.

L'insieme delle soluzioni dell'equazione lineare omogenea~\eqref{eq:edo_lineare_omogenea_ordine_n}
è un sottospazio vettoriale di $C^n(A)$ di dimensione $n$.

L'insieme delle soluzioni dell'equazione non omogenea~\eqref{eq:edo_lineare_ordine_n} è un sottospazio affine di $C^n(A)$ di dimensione $n$ parallelo al sottospazio delle soluzioni dell'equazione omogenea associata~\eqref{eq:edo_lineare_omogenea_ordine_n}. In particolare se $u_0$ è una soluzione particolare dell'equazione non omogena ogni altra soluzione $u$ si scrive nella forma
\[
  u = u_0 + v
\]
con $v$ soluzione dell'equazione omogenea associata.
\end{theorem}
%
\begin{proof}
Innanzitutto il teorema~\ref{th:edo_esistenza_globale} di esistenza globale garantisce che
le soluzioni delle equazioni~\eqref{eq:edo_lineare_ordine_n}
e~\eqref{eq:edo_lineare_omogenea_ordine_n} esistono e sono funzioni in $C^n(A)$.

Possiamo riscrivere l'equazione~\eqref{eq:edo_lineare_ordine_n} nella forma
\[
  L(u) = b
\]
con
\[
  L(u) = u^{(n)} - \sum_{k=0}^{n-1} a_k u^{(k)}.
\]
L'equazione omogenea~\eqref{eq:edo_lineare_omogenea_ordine_n}
risulta quindi essere
\[
  L(u) = 0.
\]
Si osservi che $L\colon C^n(A) \to C^0(A)$ è un operatore lineare in quanto la somma, la derivata e la moltiplicazione per una funzione sono operatori lineari sullo spazio vettoriale delle funzioni.
Dunque l'insieme delle soluzioni dell'equazione omogenea non è altro che $\ker L$ che notoriamente è uno spazio vettoriale. Cerchiamo ora di determinare la dimensione di tale spazio, mettendo in corrispondenza le soluzioni dell'equazione con un loro dato iniziale.

Fissiamo $x_0 \in I$ e $\vec y\in \RR^n$ e consideriamo
la soluzione $u$ del problema di Cauchy
\[
\begin{cases}
  L u = 0 \\
  u(x_0) = y_1 \\
  u'(x_0) = y_2 \\
  \vdots \\
  u^{(n-1)}(x_0) = y_n
\end{cases}
\]
 con dato iniziale $\vec y = (y_1,\dots,y_n)$.
 La soluzione $u$ dipenderà dal dato iniziale $\vec y$, dunque possiamo considerare $T\colon \RR^n\to C^n(A)$ l'operatore che ad ogni dato iniziale $\vec y$ associa la
corrispondente soluzione $u=T(\vec y)$. E' facile osservare che essendo l'equazione differenziale lineare omogenea, l'operatore $T$ risulta essere lineare. Infatti se $u = T(\vec y)$ e $v=T(\vec z)$ e $\lambda,\mu \in \RR$, allora
$\lambda u + \mu v$ è certamente soluzione dell'equazione differenziale, bisogna solo controllare quale dato iniziale soddisfa. Ma si ha
\[
 (\lambda u + \mu v)(x_0) = \lambda u(x_0) + \mu v(x_0) = \lambda \vec y + \mu \vec z.
\]
Dovrà quindi essere $\lambda u + \mu v = T(\lambda y + \mu \vec z)$.

Dunque $T$ è un operatore lineare.
L'operatore $T$ è iniettivo in quanto in generale $T(\vec y)(x_0) = \vec y$ e quindi se fosse $T(\vec y) = u = 0$ si avrebbe anche $u(x_0) = u'(x_0) = \dots = u^{(n-1)}(x_0) = 0$ da cui $\vec y=0$. Dunque $T\colon \RR^n \to \ker L$ è un operatore lineare iniettivo. E' anche suriettivo in quanto
per ogni $u \in \ker L$ si può scegliere $\vec y = (u(x_0), u'(x_0), \dots, u^{(n-1)}(x_0))$ e, per il teorema di esistenza e unicità, possiamo quindi osservare che $T(\vec y) = u$.
Dunque $T \colon \RR^n \to \ker L$ è un isomorfismo di spazi vettoriali. Risulta quindi che $\ker L$, cioè lo spazio delle soluzioni dell'equazione omogenea, ha dimensione $n$.

Per quanto riguarda l'equazione non omogenea
sia $V = \{ v\in C^n(A)\colon L v = b\}$ l'insieme di tutte le soluzioni. Se consideriamo una soluzione particolare $v_0\in V$ e se $v\in V$ è una qualunque altra soluzione, si osserva che
\[
  L(v-v_0) = L(v) - L(v_0) = b-b = 0.
\]
Significa che $u=v-v_0$ è soluzione dell'equazione omogenea associata: $u\in \ker L$. Dunque ogni soluzione $v$ dell'equazione non omogenea si può scrivere nella forma $v = v_0 + u$ con $v_0$ soluzione particolare della non omogenea e $u$ soluzione generale dell'equazione omogenea associata ovvero
\[
  V = v_0 + \ker L.
\]
\end{proof}

\begin{theorem}[maggiore regolarità delle soluzioni]
Se $u(x)$ è una soluzione dell'equazione differenziale lineare~\eqref{eq:edo_lineare_ordine_n} e se i coefficienti $a_1, \dots, a_{n-1}, b$ sono funzioni di classe $C^m$ per un certo $m\in \NN$ allora la soluzione è di classe $C^{m+n}$. In particolare se i coefficienti sono di classe $C^\infty$ le soluzioni sono anch'esse di classe $C^\infty$.
\end{theorem}
%
\begin{proof}
Se $u$ è soluzione di~\eqref{eq:edo_lineare_ordine_n} possiamo scrivere l'equazione in forma normale per ottenere:
\[
  u^{(n)} = b(x) - \sum_{k=0}^{n-1}a_k(x) u^{(k)}(x).
\]
Se $u$ è di classe $C^j$ allora $u^{(k)}$ è di classe $C^{j-k}$ e per $k=0,\dots, n-1$ risulta essere almeno di classe $C^{j-n+1}$ se $j-n+1\ge m$ allora il lato destro dell'equazione precedente è di classe $C^{j-n+1}$. Dunque $u^{(n)}$ è di classe $C^{j-n+1}$ il che significa che $u$ è di classe $C^{j+1}$. Risulta quindi che la regolarità di $u$ può essere incrementata di una unità alla voltà fintantoché rimane verificata $j-n+1\ge m$.
Quando $j=m+n-1$ si potrà dunque ottenere che $u$ è di classe $j+1 = m+n$ e non si potrà procedere oltre.

Se i coefficienti sono di classe $C^\infty$ il procedimento non termina mai e si ottiene dunque che anche $u$ è di classe $C^\infty$.
\end{proof}

\section{equazioni lineare di ordine $n$ a coefficienti costanti}

Una equazione differenziale ordinaria di ordine $n$ a coefficienti costanti è una equazione del tipo:
\begin{equation}\label{eq:edo_lineare_omogenea_coeff_costanti}
   \sum_{k=0}^n a_k u^{(k)}(x) = 0
\end{equation}
con $a_k\in \RR$.
Senza perdita di generalità possiamo supporre che sia $a_n\neq 0$ cosicchè tale equazione può essere scritta in forma normale e rientra nella casistica generale che abbiamo considerato nel capitolo precedente.
In particolare sappiamo che lo spazio delle soluzioni è uno spazio vettoriale di dimensione $n$.
Osserviamo che il teorema di esistenza e unicità globale garantisce che le soluzioni dell'equazione differenziale lineare a coefficienti costanti siano funzioni di classe $C^n$ definite su tutto $\RR$. Ma i coefficienti costanti
sono di classe $C^\infty$ dunque la maggiore regolarità delle soluzioni ci dice, in questo caso, che le soluzioni dovranno essere funzioni di classe $C^\infty$.

Per motivi puramente algebrici sarà utile considerare le funzioni a valori complessi.
Ricordiamo che se $u(x)$ è una funzione a valori complessi,
allora si può scrivere $u(x) = f(x) + i g(x)$ con $f$ e $g$ funzioni a valori reali.
Si definisce allora $u'(x) = f'(x) + ig'(x)$.
Denotiamo con $D \colon C^\infty(\RR, \CC)\to C^\infty(\RR, \CC)$ l'operatore derivata: $D u = u'$.

Se $P$ è un polinomio di grado $n$:
\[
  P(z) = \sum_{k=0}^n a_k z^k
\]
possiamo definire l'operatore
$P(D)\colon C^\infty(\RR,\CC) \to C^\infty(\RR,\CC)$
come
\[
  P(D)u = \sum_{k=0}^n a_k D^k u = \sum_{k=0}^n a_k u^{(k)}.
\]

In particolare l'equazione lineare omogenea a coefficienti costanti~\eqref{eq:edo_lineare_omogenea_coeff_costanti}
si può scrivere più espressivamente nella forma
\[
  P(D)u = 0
\]
Se l'equazione è in forma normale si avrà $\deg P = n$ (in quanto il cofficiente $a_n$ del termine di grado massimo è $1$ per le equazioni in forma normale).

Vogliamo ora mostrare come una decomposizione del polinomio porta ad una decomposizione delle soluzioni dell'equazione differenziale.

\begin{theorem}[isomorfismo tra polinomi e operatori differenziali a coefficienti costanti]
\label{th:6822095}
Siano $P$ e $Q$ due polinomi e sia $\lambda \in \RR$.
Allora si ha
\[
  (P\cdot Q)(D) = P(D) \circ Q(D)
\]
cioè: l'operatore differenziale associato al prodotto dei polinomi è la composizione degli operatori associati ai singoli fattori.
\end{theorem}
\begin{proof}
Sia
\[
  P(z) = \sum_{k=0}^n a_k z^k, \qquad Q(z) = \sum_{j=0}^m b_j z^j.
\]
Allora si ha
\begin{align*}
 (P\cdot Q)(z)
  &= \enclose{\sum_{k=0}^n a_k z^k Q(t)} \cdot \enclose {\sum_{j=0}^m b_j z^j} \\
  &= \sum_{k=0}^n \sum_{j=0}^m a_k b_j z^{k+j} \\
  &= \sum_{s=0}^{n+m} \enclose{\sum_{k=\max\{0,s-m\}}^{\min\{s,n\}} a_k b_{s-k}} z^s.
\end{align*}
Ma, d'altro canto, se $u\in C^\infty(\RR,\CC)$ si ha
\begin{align*}
  (P(D) \circ Q(D)) (u) &= P(D)(Q(D)(u)) \\
  &= \sum_{k=0}^n a_k D^k\enclose{\sum_{j=0}^m b_j D^j u}
  = \sum_{k=0}^n \sum_{j=0}^m a_k b_j D^{k+j} u \\
  &= \sum_{s=0}^{m+n} \enclose{\sum_{k=\max\{0,s-m\}}^{\min\{s,n\}} a_k b_{s-k}} D^s u \\
  &= (P\cdot Q)(D)(u).
\end{align*}
\end{proof}

\begin{theorem}
Sia $P$ un polinomio e sia $\lambda\in \CC$ una radice di $P$ con molteplicità $m$. Allora se $p(x)$ è un polinomio (a coefficienti complessi) di grado inferiore a $m$, la funzione $u(x) = p(x) e^{\lambda x}$ è soluzione (complessa) dell'equazione differenziale
\[
   P(D) u = 0.
\]
\end{theorem}
%
\begin{proof}
Se il polinomio $P(t)$ ha una radice $\lambda$ con molteplicità $m$ significa che $P(t)$ è divisibile per $(t-\lambda)^m$ cioè esiste un polinomio $R(t)$ tale che
\[
  P(t) = R(t)\cdot (t-\lambda)^m.
\]

Ma allora, in base al teorema~\ref{th:6822095} possiamo decomporre anche l'equazione differenziale:
\[
 P(D) u = R(D) (D - \lambda)^m u.
\]
Se $u(x) = p(x) e^{\lambda x}$ si ha
\begin{align*}
  (D-\lambda) u(x) &= Du(x) -\lambda u(x) =
  p'(x) e^{\lambda x} + p(x) \lambda e^{\lambda x} - \lambda p(x) e^{\lambda x} \\
  &= p'(x) e^{\lambda x},
\end{align*}
da cui
\[
  (D-\lambda)^m u(x) = p^{(m)}(x) e^{\lambda x}.
\]
Visto che la derivata di un polinomio è un polinomio di grado inferiore, se il polinomio $p$ ha grado inferiore a $m$ risulta che $p^{(m)}=0$. Dunque, come volevamo dimostrare,
\[
 P(D) u = R(D) (D-\lambda)^m u = R(D) 0 = 0.
\]
\end{proof}

\begin{theorem}[indipendenza delle soluzioni fondamentali]
Il sottoinsieme $\B$ di $\C^\infty(\RR, \CC)$ dato dalle funzioni
\[
   \B = \{ u\in C^\infty(\RR,\CC)\colon u(x) = x^m e^{\lambda x}, m\in \NN, \lambda \in \CC\}
\]
è indipendente.
\end{theorem}
%
\begin{proof}
Si tratta di mostrare che se una combinazione lineare finita di tali funzioni è identicamente nulla, allora tutti i coefficienti sono nulli.

Supponiamo per assurdo che esistano
$u_1, \dots, u_N\in \B$
\[
  u_k(x) = x^{m_k} e^{\lambda_k x}, \qquad k=1, \dots, N
\]
tali che
\begin{equation}\label{eq:4656978}
  \sum_{k=1}^N c_k x^{m_k} e^{\lambda_k x} = 0
  \qquad \forall x \in \RR
\end{equation}
per una qualche scelta di coefficienti $c_k \in \CC$ non tutti nulli.
Possiamo anche supporre che $N$ sia il più piccolo possibile, cioè che tutti i sottoinsiemi di $\B$ con meno di $N$ elementi risultano invece essere indipendenti.

Come primo passo vogliamo dimostrare che i coefficienti $\lambda_k$ sono tutti uguali.
Consideriamo un generico $\lambda \in \CC$ e un $m\in \NN$ e applichiamo
 l'operatore $(D-\lambda)^m$ ad ambo i membri di
\eqref{eq:4656978}.
Osserviamo che si ha
\[
  (D-\lambda) \enclose{x^{m_k} e^{\lambda_k x}}
   = ((\lambda_k-\lambda)x^{m_k} + m_k x^{m_k-1}e^{\lambda_k x}.
\]
Ora se $\lambda_k = \lambda$
ad ogni applicazione dell'operatore $D-\lambda$ il polinomio monomio $x^{m_k}$ diminuisce di grado (viene rimpiazzato dalla sua derivata). Se $m>\deg p_k$ tale termine si annulla e il corrispondente termine viene quindi rimosso dalla somma.
Dunque scegliendo $\lambda = \lambda_1$ e $m$ sufficientemente grande
si ottiene una combinazione lineare nulla di un numero minore di termini.
Osserviamo che se esistono dei $\lambda_k\neq \lambda$ i corrispondenti addendi non si annullano tutti in quanto il polinomio
\[
  (\lambda_k-\lambda)x^{m_k}(x) + m_k x^{m_k-1}(x)
\]
ha lo stesso grado del monomio $x^{m_k}$ e dunque il monomio di grado massimo viene certamente preservato.
Dunque se i coefficienti $\lambda_k$ non sono tutti uguali è possibile ridurre l'insieme di funzioni dipendenti ad un insieme più piccolo ma non vuoto. Questo è assurdo perché stavamo supponendo di avere già un insieme il più piccolo possibile.

L'unica possibilità rimanente è che tutti i coefficienti $\lambda_k$ siano uguali: $\lambda_k = \lambda$.
Ma allora avremo:
\[
  \sum_{k=1}^N c_k x^{m_k} e^{\lambda x} = 0.
\]
Moltiplicando ambo i membri per $e^{-\lambda x}$ si ottiene
\[
  \sum_{k=1}^N c_k x^{m_k} = 0
\]
e per il principio di identità dei polinomi questo significa che ogni $c_k$ è nullo: assurdo.
\end{proof}

\begin{theorem}
Sia $\lambda = \alpha + i \beta$ con $\alpha,\beta\in \RR$. Posto
\begin{gather*}
  u_1(x) = e^{\lambda x}, \qquad
  u_2(x) = e^{\bar\lambda x}\\
  v_1(x) = e^{\alpha x}\cos(\beta x), \qquad
  v_2(x) = e^{\alpha x}\sin(\beta x)
\end{gather*}
si ha che lo spazio generato dalle combinazioni lineari
a coefficienti in $\CC$ di $u_1$ e $u_2$ coincide con lo spazio generato dalle combinazioni lineari a coefficienti in $\CC$ di $v_1$ e $v_2$:
\[
\langle u_1, u_2\rangle_\CC = \langle v_1,v_2\rangle_\CC.
\]

Inoltre presa una qualunque combinazione lineare
\[
  u(x) = c_1 u_1(x) + c_2 u_2(x)
\]
con $c_1, c_2\in \CC$, si ha che $u(x)$ è reale per ogni $x\in \RR$ se e solo se
\[
  u(x) = d_1 v_1(x) + d_2 v_2(x)
\]
con $d_1, d_2\in \RR$.
\end{theorem}
%
\begin{proof}
Osserviamo che per la formula di Eulero si ha
\begin{align*}
  u_1(x) &= e^{\lambda x} = e^{\alpha x}(\cos(\beta x) + i \sin(\beta x)) = v_1(x) + i v_2(x) \\
  u_2(x) &= e^{\bar \lambda x} = e^{\alpha x}(\cos(\beta x) - i \sin(\beta x )) = v_1(x) - i v_2(x)
\end{align*}
dunque $u_1$ e $u_2$ stanno nello spazio generato da $v_1$ e $v_2$ e di conseguenza l'intero spazio $U$ generato da $u_1$ e $u_2$ è contenuto nello spazio $V$ generato da $v_1$ e $v_2$. D'altra parte abbiamo già visto che $u_1$ e $u_2$ sono indipendenti, dunque $\dim U = 2$. Certamente $\dim V \le 2$ (essendo $V$ generato da due vettori) e si conclude quindi che $U=V$. In particolare ne consegue che anche $v_1$ e $v_2$ sono indipendenti.

Dunque se $u$ è combinazione complessa di $u_1$ e $u_2$ potremo scrivere
\[
  u(x) = d_1 v_1(x) + d_2 v_2(x)
\]
con $d_1,d_2$ opportuni coefficienti complessi. Osserviamo ora che se $d_1$ e $d_2$ sono reali allora certamente $u(x)\in \RR$ in quanto $v_1(x),v_2(x)\in \RR$ per ogni $x\in \RR$.
Viceversa supponiamo che $u(x)\in \RR$ per ogni $x\in \RR$. Allora essendo $v_1(x),v_2(x)\in \RR$ si ha
\[
  0 = \Im u(x) = (\Im d_1) v_1(x) + (\Im d_2) v_2(x)
\]
e visto che $v_1$ e $v_2$ sono indipendenti, una combinazione lineare nulla può solo aversi con coefficienti nulli: ne consegue che $\Im d_1 = \Im d_2 = 0$ ovvero che $d_1,d_2\in \RR$.
\end{proof}

I risultati precedenti ci permettono di determinare tutte le soluzioni delle equazioni lineari omogenee a coefficienti costanti come nel seguente esempio.

\begin{example}
Si determinino tutte le soluzioni dell'equazione differenziale
\[
  u^{(5)}(x) + 2 u'''(x) + u'(x) = 0.
\]
\end{example}
%
\begin{proof}[Svolgimento]
L'equazione può essere scritta nella forma
\[
  P(D) u = 0
\]
con $P(t) = t^5 + 2 t^3+t$. Possiamo fattorizzare il polinomio $P$ nel campo complesso:
\[
t^5 + 2t^3 +t = t(t^2+1)^2 = t (t+i)^2(t-i)^2.
\]
Il polinomio ha una radice $\lambda_0=0$ con molteplicità uno e due radici complesse coniugate $\lambda_1 = i$, $\lambda_2=-i$ con molteplicità due.
Risulta quindi che le seguenti funzioni devono essere soluzione complesse dell'equazione:
\[
  e^x, \qquad
  e^{ix}, \qquad
  x e^{ix}, \qquad
  e^{-ix}, \qquad
  x e^{-ix}.
\]
Per linearità ogni combinazione lineare complessa di queste soluzioni è ancora una soluzione complessa.
Visto che queste funzioni sono linearmente indipendenti, lo spazio generato ha dimensione $5$, come l'ordine dell'equazione.
Possiamo però sostituire le due funzioni $e^{ix}$ e $e^{-ix}$ con le funzioni reali $\cos x$ e $\sin x$ e lo spazio generato non cambia. Allo stesso modo possiamo sostiture la coppia di funzioni $x e^{ix}$ e $xe^{-ix}$ con $x\cos x$ e $x\sin x$ senza variare lo spazio generato. Risulta quindi che ogni combinazione lineare complessa delle seguenti funzioni reali:
\[
  e^x, \qquad
  \cos x, \qquad
  \sin x, \qquad
  x \cos x, \qquad
  x \sin x
\]
è una soluzione dell'equazione differenziale. Inoltre le combinazioni lineari a coefficienti reali generano uno spazio vettoriale reale di soluzioni che dovrà ancora avere dimensione $5$. Sappiamo però che lo spazio di tutte le soluzioni ha dimensione $5$ (pari all'ordine dell'equazione) dunque lo spazio che abbiamo determinato esaurisce tutte le soluzioni. Ogni soluzione reale si potrà dunque scrivere nella forma:
\[
  u(x) = c_1 e^x + c_2 \cos x + c_3 \sin x + c_4 x \cos x + c_5 x \sin x
\]
con $c_1, c_2, \dots, c_5 \in \RR$ costanti arbitrarie.
\end{proof}

Possiamo in effetti dare un enunciato generale.

\begin{theorem}[soluzioni dell'equazione lineare omogenea a coefficienti costanti]
Se $P(t)$ è un polinomio a coefficienti reali.
Ogni soluzione $u\colon \RR\to \RR$ dell'equazione differenziale
\[
  P(D) u = 0
\]
si scrive nella forma
\[
  u(x) = \sum_{k=1}^N \sum_{j=1}^{n_k} c_{kj} x^j e^{\lambda_k x}
        + \sum_{k=1}^M \sum_{j=1}^{m_l} x^j e^{\alpha_k x} (a_{kj} \cos(\beta_k x) + b_{kj}\sin(\beta_k x))
\]
dove $\lambda_1, \dots, \lambda_N$ sono le radici reali del polinomio $P(t)$, $n_1, \dots, n_N$ sono le rispettive molteplicità,
$\alpha_k \pm i \beta_k$ sono le radici complesse coniugate (non reali) del polinomio $P$ con $k=1,\dots, M$ ognuna con molteplicità $m_k$ e infine $c_{kj}, a_{kj}, b_{kj}\in \RR$ sono costanti arbitrarie.
\end{theorem}
